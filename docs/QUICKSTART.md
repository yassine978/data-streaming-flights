# Quick Start Reference

**Project:** Real-Time Flight Data Streaming  
**Status:** âœ… Complete & Ready to Run  
**Members:** Yassine (Member 1) | Ramy (Member 2) | Chiheb (Member 3)  

---

## ğŸš€ One-Minute Setup

```bash
# 1. Setup environment
cd data-streaming-flights
python -m venv venv
source venv/bin/activate                    # Linux/Mac
# venv\Scripts\activate.bat                  # Windows

# 2. Install dependencies
pip install -r producer/requirements.txt
pip install -r spark_streaming/requirements.txt
pip install -r dashboard/requirements.txt

# 3. Configure credentials (.env file)
cp .env.example .env
# Edit .env with your OpenSky credentials

# 4. Start Kafka
cd docker && docker-compose up -d && cd ..

# Done! Now run the three components in separate terminals...
```

---

## ğŸ¯ Running the System (3 Terminal Windows)

### Terminal 1: Start Producer (Data Ingestion)
```bash
python producer/api_producer.py
```
**Expected output:**
```
[2026-01-15 10:30:00] Connecting to OpenSky Network...
[2026-01-15 10:30:02] Connected! âœ“
[2026-01-15 10:30:15] Published 42 flights to flight-raw-data
[2026-01-15 10:30:30] Published 38 flights to flight-raw-data
```

### Terminal 2: Start Spark Streaming (Processing & ML)
```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql_2.12:3.5.0 \
  --master local[*] \
  spark_streaming/stream_processor.py
```
**Expected output:**
```
[2026-01-15 10:31:00] Starting Spark Stream Processing...
[2026-01-15 10:31:05] Spark Session initialized
[2026-01-15 10:31:10] Reading from flight-raw-data
[2026-01-15 10:31:15] Batch 1 processed: 40 records
```

### Terminal 3: Start Dashboard (Visualization)
```bash
streamlit run dashboard/app.py
```
**Expected output:**
```
You can now view your Streamlit app in your browser.
Local URL: http://localhost:8501
```

---

## ğŸ“Š Access Dashboard

Open browser: **http://localhost:8501**

### Dashboard Sections
- **Top Left:** 5 KPI Metric Cards
  - Total Events
  - Anomalies Detected
  - Avg Velocity
  - Avg Altitude  
  - Active Aircraft Count

- **Charts:** 7 Interactive Visualizations
  - Velocity Timeline
  - Altitude Timeline
  - Anomaly Scatter (2D)
  - Windowed Metrics Trend
  - Anomaly Count Trend
  - Aircraft Distribution (Top 15)
  - Flight Phase Distribution (Pie)

- **Data Tables:** 3 Views
  - Recent Processed Data (last 20 events)
  - Recent Anomalies (last 10 anomalies)
  - Aggregated Window Statistics

- **Sidebar:** Configuration Controls
  - Refresh Interval slider (1-10 seconds)
  - Auto Refresh toggle
  - Data Buffer Size slider
  - Kafka connection settings

---

## âœ… Verification Checklist

```bash
# 1. Check Kafka is running
docker-compose ps
# OUTPUT: kafka and zookeeper should show "Up"

# 2. Check producer is publishing
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flight-raw-data \
  --max-messages 1

# 3. Check spark is processing
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flight-processed-data \
  --max-messages 1

# 4. Check dashboard loads
curl http://localhost:8501
```

---

## ğŸ“ File Structure Quick Reference

```
Producer (Member 1):
â”œâ”€â”€ producer/api_producer.py      # Main producer logic
â”œâ”€â”€ producer/opensky_client.py    # OpenSky API client
â”œâ”€â”€ producer/config.py            # Configuration
â””â”€â”€ producer/requirements.txt     # Dependencies

Spark Streaming (Member 2):
â”œâ”€â”€ spark_streaming/stream_processor.py    # Main Spark job
â”œâ”€â”€ spark_streaming/transformations.py     # Data cleaning & features
â”œâ”€â”€ spark_streaming/ml_inference.py        # ML model inference
â”œâ”€â”€ spark_streaming/aggregations.py        # Windowing
â””â”€â”€ spark_streaming/requirements.txt       # Dependencies

Models (Member 2):
â”œâ”€â”€ models/flight_anomaly_detector.pkl   # Trained ML model
â”œâ”€â”€ models/flight_scaler.pkl             # Feature scaler
â””â”€â”€ models/flight_features.json          # Feature list

Dashboard (Member 3):
â”œâ”€â”€ dashboard/app.py                          # Main Streamlit app
â”œâ”€â”€ dashboard/components/
â”‚   â”œâ”€â”€ kafka_consumer.py                    # Kafka integration
â”‚   â”œâ”€â”€ metrics.py                           # KPI metrics
â”‚   â””â”€â”€ charts.py                            # Chart visualizations
â””â”€â”€ dashboard/requirements.txt               # Dependencies

Documentation (Member 3):
â”œâ”€â”€ MEMBER3_README.md            # Member 3 comprehensive guide
â”œâ”€â”€ docs/ARCHITECTURE.md         # System design & architecture
â”œâ”€â”€ docs/DEPLOYMENT.md           # Deployment instructions
â”œâ”€â”€ docs/API_REFERENCE.md        # Data schemas & API docs
â””â”€â”€ docs/QUICKSTART.md           # This file!

Tests:
â”œâ”€â”€ tests/test_producer.py       # Producer tests
â”œâ”€â”€ tests/test_spark.py          # Spark tests
â”œâ”€â”€ tests/test_integration.py    # Integration tests

Docker:
â”œâ”€â”€ docker/docker-compose.yml    # Kafka infrastructure
â””â”€â”€ docker/.env                  # Docker environment variables
```

---

## ğŸ”§ Troubleshooting Quick Fixes

### Dashboard Shows No Data
```bash
# 1. Check producer is running
ps aux | grep api_producer.py
# If empty, start: python producer/api_producer.py

# 2. Check Spark is running
ps aux | grep spark
# If empty, start spark-submit command (see above)

# 3. Check Kafka topics have data
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flight-processed-data \
  --from-beginning --max-messages 5
```

### Kafka Connection Error
```bash
# Restart Kafka
docker-compose -f docker/docker-compose.yml restart kafka

# Wait 10 seconds and check status
docker-compose -f docker/docker-compose.yml ps
```

### Out of Memory
```bash
# Increase Spark memory
spark-submit \
  --driver-memory 4g \
  --executor-memory 4g \
  spark_streaming/stream_processor.py

# Or reduce dashboard buffer size (sidebar slider)
```

### Python Module Not Found
```bash
# Reinstall dependencies
pip install -r producer/requirements.txt
pip install -r spark_streaming/requirements.txt
pip install -r dashboard/requirements.txt
```

---

## ğŸ“Š Data Flow Diagram

```
OpenSky API (every 15s)
        â†“
[Producer] (Member 1)
        â†“
Kafka: flight-raw-data
        â†“
[Spark Streaming] (Member 2)
â”œâ”€â”€ Cleaning
â”œâ”€â”€ Feature Engineering
â”œâ”€â”€ ML Anomaly Detection
â””â”€â”€ Windowed Aggregations
        â†“
Kafka: flight-processed-data
Kafka: flight-aggregated-data
        â†“
[Streamlit Dashboard] (Member 3)
        â†“
Browser: http://localhost:8501
```

---

## ğŸ› ï¸ Common Commands

### Start Services
```bash
# Start everything (must use separate terminals)
python producer/api_producer.py &
spark-submit --packages ... spark_streaming/stream_processor.py &
streamlit run dashboard/app.py
```

### Stop Services
```bash
# Kill by process name
pkill -f api_producer.py
pkill -f spark
lsof -i :8501 | grep LISTEN | awk '{print $2}' | xargs kill -9

# Or stop Docker
docker-compose down
```

### Check Logs
```bash
# Kafka
docker logs kafka

# Producer (if started in terminal)
tail -f producer.log

# Spark (if running locally)
tail -f spark.log

# Dashboard
ps aux | grep streamlit  # Find process
tail -f /tmp/streamlit-logs/
```

### Monitor Kafka Topics
```bash
# List all topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Monitor topic (follow latest messages)
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flight-processed-data

# Get topic details
docker exec kafka kafka-topics \
  --describe \
  --bootstrap-server localhost:9092 \
  --topic flight-processed-data
```

---

## ğŸ“ Configuration Defaults

### Producer (Member 1)
- **API Poll Interval:** 15 seconds
- **Kafka Topic:** flight-raw-data
- **Retry Attempts:** 3
- **API Timeout:** 10 seconds

### Spark (Member 2)
- **Window Size:** 10 minutes
- **Watermark:** 5 minutes
- **Micro-batch Interval:** 10 seconds
- **ML Model:** Isolation Forest (trained)

### Dashboard (Member 3)
- **Default Refresh:** 3 seconds
- **Refresh Range:** 1-10 seconds
- **Data Buffer:** 500 records
- **Kafka Consumer Timeout:** 1 second

---

## ğŸ”— Important URLs & Ports

| Service | URL | Default Port |
|---------|-----|--------------|
| Dashboard | http://localhost:8501 | 8501 |
| Kafka | localhost:9092 | 9092 |
| Zookeeper | localhost:2181 | 2181 |
| Spark UI | http://localhost:4040 | 4040 |

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `README.md` | Main project overview |
| `MEMBER3_README.md` | Complete Member 3 documentation |
| `docs/ARCHITECTURE.md` | System design & architecture (600+ lines) |
| `docs/DEPLOYMENT.md` | Deployment guide for all environments |
| `docs/API_REFERENCE.md` | Kafka topics, data schemas, APIs |
| `PROJECT_STRATEGY_COMPLETE.md` | Overall project strategy & phases |
| `README_Yassine.md` | Member 1: Data ingestion details |
| `Ramyreadme.md` | Member 2: Stream processing & ML details |

---

## âœ¨ Key Features at a Glance

âœ… **Real-Time Ingestion**
- 450 flights/min from OpenSky API
- 15-second polling interval
- OAuth2 authentication

âœ… **Stream Processing**
- Apache Spark Structured Streaming
- 15+ derived features
- Windowed aggregations (10-min)

âœ… **Machine Learning**
- Isolation Forest anomaly detection
- 100% F1-score on test data
- Real-time inference via Spark UDFs

âœ… **Visualization**
- 7 interactive Plotly charts
- 5 KPI metric cards
- 3 data views
- Auto-refresh (configurable 1-10s)

âœ… **Production Ready**
- Error handling & retries
- Comprehensive logging
- Configuration management
- Docker containerization

---

## ğŸ¯ Next Steps

1. **Run the System** â†’ Follow "Running the System" section above
2. **Access Dashboard** â†’ Open http://localhost:8501
3. **Monitor Performance** â†’ Use sidebar controls & watch metrics
4. **Run Tests** â†’ `pytest tests/test_integration.py -v`
5. **Deploy** â†’ See `docs/DEPLOYMENT.md` for production setup

---

## ğŸ’¡ Tips & Tricks

### Adjust Dashboard Refresh
Use the sidebar slider to change refresh interval from 1-10 seconds. Faster updates = higher latency, higher lag.

### View Raw Kafka Data
```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic flight-processed-data \
  --property print.key=true --property key.separator=":"
```

### Monitor System Performance
```bash
# CPU/Memory usage
top
# or on Windows: Task Manager

# Kafka lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --group flight-dashboard-group \
  --describe
```

### Replay Historical Data
Change `auto_offset_reset` in dashboard config from "latest" to "earliest" to see all historical data.

---

## ğŸ“ Support & Documentation

**For Issues:**
- Producer Issues â†’ Check `README_Yassine.md`
- Spark Issues â†’ Check `Ramyreadme.md`  
- Dashboard Issues â†’ Check `MEMBER3_README.md`
- Architecture Questions â†’ See `docs/ARCHITECTURE.md`
- Deployment Help â†’ See `docs/DEPLOYMENT.md`
- Data Schema Questions â†’ See `docs/API_REFERENCE.md`

**Quick Links:**
- Project GitHub: [Repository URL]
- OpenSky Network: https://opensky-network.org
- Apache Kafka Docs: https://kafka.apache.org
- Streamlit Docs: https://docs.streamlit.io
- Spark Docs: https://spark.apache.org/docs

---

## ğŸ‰ You're All Set!

The system is fully implemented and ready to run. All three components (Producer, Spark, Dashboard) are complete and integrated.

**Happy streaming!** âœˆï¸
